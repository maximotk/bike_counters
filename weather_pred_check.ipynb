{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ydata_profiling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from feature_engineering import codify_date, codify_date_2, remove_outliers, get_X_y, covid_19, covid_19_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "test = pd.read_parquet(Path(\"data\") / \"final_test.parquet\")\n",
    "\n",
    "test_old = test.copy()\n",
    "\n",
    "weather_data = pd.read_csv(\"data/external_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter_id', 'counter_name', 'site_id', 'site_name', 'date',\n",
       "       'counter_installation_date', 'coordinates', 'counter_technical_id',\n",
       "       'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_old.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = codify_date_2(df)\n",
    "test = codify_date_2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MSE scores from cross-validation: [-0.96288667 -0.55461495 -0.64306749 -0.95816682 -1.43161343]\n",
      "Average Negative MSE: -0.9100698711407886\n"
     ]
    }
   ],
   "source": [
    "# Extract the relevant columns from your DataFrame\n",
    "features = ['hour', 'month', 'IsHoliday', 'day', \"day_of_week\", \"is_weekend\"]\n",
    "target = 'log_bike_count'\n",
    "\n",
    "# Extract feature matrix (X) and target vector (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Instantiate the HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor(max_iter=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation to evaluate the performance of the model\n",
    "# Use neg_mean_squared_error for regression tasks\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Output the negative mean squared error (MSE) scores for each fold\n",
    "print(\"Negative MSE scores from cross-validation:\", cv_scores)\n",
    "print(\"Average Negative MSE:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  log_bike_count\n",
      "0   0        1.471516\n",
      "1   1        4.456159\n",
      "2   2        5.158623\n",
      "3   3        4.210475\n",
      "4   4        3.285660\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor(max_iter=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract the relevant columns from df_test (test data) for making predictions\n",
    "X_test = test[features]\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Create the output DataFrame with \"Id\" (the index) and the \"log_bike_count\" predictions\n",
    "output_df = pd.DataFrame({\n",
    "    'Id': test.index,  # Assuming Id is just the index of the test dataframe\n",
    "    'log_bike_count': y_pred_test\n",
    "})\n",
    "\n",
    "# Optionally, save the output to a CSV file for Kaggle submission\n",
    "# output_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the output dataframe\n",
    "print(output_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"submission_maxim_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now including weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('data/external_data.csv')\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "weather.drop_duplicates(inplace=True)\n",
    "\n",
    "# let's consider the \"nearest preceding weather\" --> biking behavior is influenced by previous weather; incorporating future information could introduce bias\n",
    "# therefore: consider the \"last 3h\" variables (for consistency)\n",
    "\n",
    "relevant_columns = [\n",
    "    'date',         # Date\n",
    "    't',            # Temperature\n",
    "    #'tn12',         # Minimum Temperature over 12 hours\n",
    "    #'tn24',         # Minimum Temperature over 24 hours\n",
    "    #'tx12',         # Maximum Temperature over 12 hours\n",
    "    #'tx24',         # Maximum Temperature over 24 hours\n",
    "    #'tminsol',      # Ground Temperature\n",
    "    'rr1',          # Precipitation in last hour\n",
    "    'rr3',          # Precipitation in last 3 hours\n",
    "    #'rr6',          # Precipitation in last 6 hours\n",
    "    #'rr12',         # Precipitation in last 12 hours\n",
    "    #'rr24',         # Precipitation in last 24 hours\n",
    "    'ht_neige',     # Total Snow Depth\n",
    "    #'ssfrai',       # Fresh Snow Depth\n",
    "    'ff',           # Wind Speed\n",
    "    'raf10',        # Wind Gusts over 10 minutes\n",
    "    'u',            # Humidity\n",
    "    #'vv',           # ok\n",
    "    'ww',           # Current Weather Condition\n",
    "    'etat_sol',     # Ground Condition\n",
    "    'tend',         # Pressure Trend in 3 hours\n",
    "    #'tend24'        # Pressure Trend in 24 hours\n",
    "]\n",
    "\n",
    "weather = weather[relevant_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = df['datetime'].astype('datetime64[ns]')\n",
    "\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "df = df.sort_values('datetime')\n",
    "weather = weather.sort_values('date')\n",
    "\n",
    "df = pd.merge_asof(df, weather, left_on='datetime', right_on='date', direction='backward', suffixes=('', '_weather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter_id', 'counter_name', 'site_id', 'site_name', 'date',\n",
       "       'counter_installation_date', 'coordinates', 'counter_technical_id',\n",
       "       'latitude', 'longitude', 'datetime', 'year', 'month', 'day',\n",
       "       'day_of_week', 'hour', 'is_weekend', 'IsHoliday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do all columns match?  True\n",
      "\n",
      "Test_old before merge (first 5 rows):\n",
      "            counter_id              counter_name    site_id  \\\n",
      "0  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "1  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "2  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "3  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "4  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "\n",
      "              site_name            datetime counter_installation_date  \\\n",
      "0  28 boulevard Diderot 2021-09-10 01:00:00                2013-01-18   \n",
      "1  28 boulevard Diderot 2021-09-10 13:00:00                2013-01-18   \n",
      "2  28 boulevard Diderot 2021-09-10 17:00:00                2013-01-18   \n",
      "3  28 boulevard Diderot 2021-09-10 19:00:00                2013-01-18   \n",
      "4  28 boulevard Diderot 2021-09-10 22:00:00                2013-01-18   \n",
      "\n",
      "          coordinates counter_technical_id   latitude  longitude  \n",
      "0  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "1  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "2  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "3  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "4  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "\n",
      "Test after merge and sorting (first 5 rows):\n",
      "               counter_id              counter_name    site_id  \\\n",
      "0     100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "719   100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "925   100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "1012  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "1184  100007049-102007049  28 boulevard Diderot E-O  100007049   \n",
      "\n",
      "                 site_name            datetime counter_installation_date  \\\n",
      "0     28 boulevard Diderot 2021-09-10 01:00:00                2013-01-18   \n",
      "719   28 boulevard Diderot 2021-09-10 13:00:00                2013-01-18   \n",
      "925   28 boulevard Diderot 2021-09-10 17:00:00                2013-01-18   \n",
      "1012  28 boulevard Diderot 2021-09-10 19:00:00                2013-01-18   \n",
      "1184  28 boulevard Diderot 2021-09-10 22:00:00                2013-01-18   \n",
      "\n",
      "             coordinates counter_technical_id   latitude  longitude  \n",
      "0     48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "719   48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "925   48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "1012  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n",
      "1184  48.846028,2.375429          Y2H15027244  48.846028   2.375429  \n"
     ]
    }
   ],
   "source": [
    "test['datetime'] = test['datetime'].astype('datetime64[ns]')\n",
    "test_old = test_old.rename(columns={'date': 'datetime'})\n",
    "\n",
    "# Step 1: Store the columns of interest from the original test_old dataset\n",
    "columns_of_interest_old = ['counter_id', 'counter_name', 'site_id', 'site_name', 'datetime',\n",
    "       'counter_installation_date', 'coordinates', 'counter_technical_id',\n",
    "       'latitude', 'longitude']\n",
    "\n",
    "# Rename 'date' column to 'datetime' in test_old to match the test_merged_sorted column\n",
    "test_old = test_old[columns_of_interest_old].copy()\n",
    "\n",
    "# Step 2: Perform the merging and sorting as before\n",
    "test['original_index'] = test.index  # Store original index\n",
    "test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "\n",
    "test_sorted = test.sort_values('datetime')\n",
    "weather_sorted = weather.sort_values('date')\n",
    "\n",
    "# Perform the merge\n",
    "test = pd.merge_asof(test_sorted, weather_sorted, left_on='datetime', right_on='date', direction='backward', suffixes=('', '_weather'))\n",
    "\n",
    "# Step 3: Sort the merged test data by the original index to restore the original order\n",
    "test = test.sort_values('original_index')\n",
    "\n",
    "# Drop the 'original_index' column after sorting\n",
    "test = test.drop(columns=['original_index'])\n",
    "\n",
    "# Step 4: Compare the columns of interest before and after the merge/sort (with test_old)\n",
    "columns_match_all_old = test_old.values == test[columns_of_interest_old].values\n",
    "\n",
    "# Step 5: Print the results\n",
    "print(\"Do all columns match? \", columns_match_all_old.all())\n",
    "print(\"\\nTest_old before merge (first 5 rows):\")\n",
    "print(test_old.head())\n",
    "print(\"\\nTest after merge and sorting (first 5 rows):\")\n",
    "print(test[columns_of_interest_old].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test[\\'datetime\\'] = test[\\'datetime\\'].astype(\\'datetime64[ns]\\')\\n\\n# Assuming test and weather are already loaded\\n# Step 1: Store the original order of the test dataset (before sorting)\\ntest[\\'original_index\\'] = test.index\\n\\n# Step 2: Sort the test dataset by \\'date\\' before merging with weather\\ntest = test.sort_values(\\'datetime\\')\\n\\n# Step 3: Merge weather data (as you did before)\\ntest = pd.merge_asof(test, weather, left_on=\\'datetime\\', right_on=\\'date\\', direction=\\'backward\\')\\n# Step 4: Revert to the original order using \\'original_index\\'\\ntest = test.sort_values(\\'original_index\\')\\n\\ntest[\"original_index\"]'"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" test['datetime'] = test['datetime'].astype('datetime64[ns]')\n",
    "\n",
    "# Assuming test and weather are already loaded\n",
    "# Step 1: Store the original order of the test dataset (before sorting)\n",
    "test['original_index'] = test.index\n",
    "\n",
    "# Step 2: Sort the test dataset by 'date' before merging with weather\n",
    "test = test.sort_values('datetime')\n",
    "\n",
    "# Step 3: Merge weather data (as you did before)\n",
    "test = pd.merge_asof(test, weather, left_on='datetime', right_on='date', direction='backward')\n",
    "# Step 4: Revert to the original order using 'original_index'\n",
    "test = test.sort_values('original_index')\n",
    "\n",
    "test[\"original_index\"]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ab hier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative MSE scores from cross-validation: [-0.95044582 -0.81106293 -0.90263914 -1.17604121 -1.10087524]\n",
      "Average Negative MSE: -0.9882128666193614\n"
     ]
    }
   ],
   "source": [
    "weather_columns = ['t', 'rr1', 'rr3', 'ff', 'raf10', 'u', 'ww', 'etat_sol', 'tend']\n",
    "\n",
    "# Extract the relevant columns from your DataFrame\n",
    "features = ['hour', 'month', 'IsHoliday', 'day', \"day_of_week\", \"is_weekend\"] + weather_columns\n",
    "target = 'log_bike_count'\n",
    "\n",
    "# Extract feature matrix (X) and target vector (y)\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Instantiate the HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor(max_iter=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation to evaluate the performance of the model\n",
    "# Use neg_mean_squared_error for regression tasks\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Output the negative mean squared error (MSE) scores for each fold\n",
    "print(\"Negative MSE scores from cross-validation:\", cv_scores)\n",
    "print(\"Average Negative MSE:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['counter_id', 'counter_name', 'site_id', 'site_name', 'date',\n",
       "       'counter_installation_date', 'coordinates', 'counter_technical_id',\n",
       "       'latitude', 'longitude', 'datetime', 'year', 'month', 'day',\n",
       "       'day_of_week', 'hour', 'is_weekend', 'IsHoliday', 'date_weather', 't',\n",
       "       'rr1', 'rr3', 'ht_neige', 'ff', 'raf10', 'u', 'ww', 'etat_sol', 'tend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  log_bike_count\n",
      "0   0        1.494013\n",
      "1   1        4.060301\n",
      "2   2        4.887337\n",
      "3   3        3.870088\n",
      "4   4        3.209452\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor(max_iter=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract the relevant columns from df_test (test data) for making predictions\n",
    "X_test = test[features]\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Create the output DataFrame with \"Id\" (the index) and the \"log_bike_count\" predictions\n",
    "output_df = pd.DataFrame({\n",
    "    'Id': range(0, 51440, 1),  # Assuming Id is just the index of the test dataframe\n",
    "    'log_bike_count': y_pred_test\n",
    "})\n",
    "\n",
    "# Optionally, save the output to a CSV file for Kaggle submission\n",
    "# output_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the output dataframe\n",
    "print(output_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"submission_maxim.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51440, 2)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
